\documentclass{article}

\input{preamble}

\newcommand\hwnumber{2}

\begin{document}

\maketitle

\begin{ex}
Suppose 
\end{ex}

\begin{ex}
The \emph{standard deviation} is a clearer measure of uncertainty, if you ask me. That tells you how far you can expect to be off the average. 

Entropy is the expected value of the order of magnitude of the likeliness of events. This order of magnitude of the likeliness of an event is a `measure' for the amount of information that event is contributing to the whole system. If there is an event which will happen with great certainty, that will not add information to what state someone might expect to be in (or, what message he might have), because he can read that off from merely looking at the probability measure and see that this event is almost certain to happen. Furthermore, in the case of two independent events, information is added according to the formula $\log(p(A\cap B))=\log(p(A))+\log(p(B))$. Thus, when two independent events are observed, information is added without further ado! Or, the occurence of both events gives the same information as the two events separately, as one would wish for a theory of information. 
\end{ex}

\end{document}
