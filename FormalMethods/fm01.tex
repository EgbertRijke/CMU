\documentclass{article}

\input{preamble}

\newcommand\hwnumber{1}

\begin{document}

\maketitle

\begin{ex}
%set local notation
\newcommand{\ulimsym}{{\uparrow}}
\newcommand{\ulim}[2]{\ulimsym_{#1\in\mathbb{N}}\,{#2}_{#1}}
\newcommand{\dlimsym}{{\downarrow}}
\newcommand{\dlim}[2]{\dlimsym_{#1\in\mathbb{N}}\,{#2}_{#1}}
Let $X$ be a space, let $\sigalg{A}$ be a $\sigma$-algebra on $X$ and
let $P:\sigalg{A}\to[0,1]$ be a finitely additive function with $P(\emptyset)=0$ and $P(X)=1$. The exercise asks to verify that the three properties
\begin{enumerate}
\item $P$ is $\sigma$-additive: let $A:=\{s:\mathbb{N}\to\sigalg{A}\mid i\neq j \Rightarrow s_i\cap s_j=\emptyset\text{ for all }i,j\in\mathbb{N}\}$ be the collection of pairwise disjoint sequences of measurable sets; for every $S\in A$ we have
\begin{equation*}
\textstyle
P(\bigcup_{i\in\mathbb{N}}S_i)=\sum_{i\in\mathbb{N}}P(S_i)
\end{equation*}
\item $P$ is upwards continuous: let $B:=\in\{s:\mathbb{N}\to\sigalg{A}\mid s_i\subseteq s_{i+1}\text{ for every }i\in\mathbb{N}\}$ be the collection of monotonously increasing sequences of measurable sets; for every $S\in B$ we have
\begin{equation*}
\textstyle
P(\bigcup_{i\in\mathbb{N}}S_i)=\ulimsym_{i}\,{P(S_i)}
\end{equation*}
\item $P$ is downwards continuous: let $C:=\{s:\mathbb{N}\to\sigalg{A}\mid s_{i+1}\subseteq s_i\text{ for every }i\in\mathbb{N}\}$ be the collection of monotonously decreasing sequences of measurable sets; for every $S\in C$ we have
\begin{equation*}
\textstyle
P(\bigcap_{i\in\mathbb{N}}S_i)=\dlim{i}{P(S_i)}
\end{equation*}
\end{enumerate}
are equivalent.

Suppose first that $P$ is $\sigma$-additive and let $S\in B$. Then we have the sequence $S':\mathbb{N}\to\sigalg{A}$ given by $S'_0:=S_0$ and $S'_{i+1}:=S_{i+1}\setminus S_i$ which is pairwise disjoint because $S$ is increasing. Since $P$ is assumed to be $\sigma$-additive, we have the equality
\begin{equation*}
\textstyle
P(\bigcup_{i\in\mathbb{N}}S_i)=\sum_{i\in\mathbb{N}}P(S_i).
\end{equation*}
By definition, we have the equality
\begin{equation*}
\sum_{i\in\mathbb{N}}P(S_i)=\lim_{j\to\infty}\sum_{i=0}^j P(S_i).
\end{equation*}
Since $P$ is assumed to take values in $[0,1]$ it is always non-negative, so the latter is a limit of a monotone increasing sequence of real numbers, which we may write in this exercise as $\ulimsym_j\,\sum_{i=0}^j P(S_i)$. Now we note that 
\begin{equation*}
\textstyle
\sum_{i=0}^j P(S_i) = P(\bigcup_{i=0}^j S_i) = P(S_j)
\end{equation*}
by the $\sigma$-additivity of $P$ ($\sigma$-additivity implies finite additivity). We conclude that $P(\bigcup_{i\in\mathbb{N}}S_i)=\ulimsym_{j}\,{P(S_j)}$.

Now suppose that $P$ is upwards continuous. We will show that it follows that $P$ is $\sigma$-additive. To begin, let $S\in A$. Then we construct the monotone increasing sequence $S'$ by $S'_i=\bigcup_{j=0}^i S_j$. Now it follows by the upwards continuity of $P$ that
\begin{equation*}
\textstyle
P(\bigcup_{i\in\mathbb{N}}S'_i)=\ulimsym_{i}\,{P(S'_i)}
\end{equation*}
Since $P$ is assumed to be finitely additive, we have
\begin{equation*}
\ulimsym_{i}\,P(S'_i)=\ulimsym_{i}\sum_{j=0}^i P(S_j).
\end{equation*}
This upwards limit is just $\sum_{i\in\mathbb{N}}P(S_i)$, proving the $\sigma$-additivity.

It remains to show that for finite measures, upwards continuity is equivalent to
downwards continuity. The finiteness condition is important here. Note that we
have a bijection $\varphi$ from $B$ to $C$ given by $\{S_i\}_i\mapsto\{X\setminus S_i\}_i$.
This bijection has the property that $\bigcup_i S_i=\bigcap_i\varphi(S)_i$. Note that
$P(S_i)=1-P(X\setminus S_i)$ for each $i\in\mathbb{N}$ and that therefore
$\ulimsym_{i}P(S_i)=\dlimsym_{i}(1-P(X\setminus S_i))$. Since $\varphi$ is a bijection,
this establishes the equivalence. 
\end{ex}

\begin{ex}
To show that $\mathcal{C}$ is a field, note first that the union of an empty collection (which is thus a finite collection) of cones is the empty set. Its complement is the cone determined by the initial segment of length $0$. To see that $\mathcal{C}$ has the other complements, let $E$ be a finite subset of $2^{<\omega}$. Denote by $l:2^{<\omega}\to\mathbb{N}$ the function which assigns to each $e\in 2^{<\omega}$ and write $L:=\max l(E)$. Now write $C_E:=\bigcup_{e\in E}[e]$ and define
\begin{equation*}
E^\ast:=\{e\in 2^L\mid \exists_{(\sigma\in C_E)}\sigma_L= e\};
\end{equation*}
here $\sigma_L$ is the initial segment of $\sigma$ of length $L$. In other words, $E^\ast$ is the image of $C_E$ of the function $r_L:2^\omega\to 2^L$ which restricts a sequence $\sigma$ to its initial segment of length $L$. Then it is immediate that
\begin{equation*}
\bigcup\nolimits_{e\in E}[e]=\bigcup\nolimits_{e\in E^\ast}[e],
\end{equation*}
and therefore it is also immediate that
\begin{equation*}
\Big(\bigcup\nolimits_{e\in E}[e]\Big)^c = \bigcup\nolimits_{e\in 2^L\setminus E^\ast}[e].
\end{equation*}
This shows that complements of finite unions of cones can be written as finite unions of cones, i.e.~that $\mathcal{C}$ has complements. Thirdly, note that finite unions of finite unions of cones are again finite unions of cones, so that $\mathcal{C}$ is also closed under finite unions. This finishes the proof that $\mathcal{C}$ is a field.

Next, suppose that the set of outcomes (i.e.~the set which was initially taken to be the set $2$) is an infinite set $X$ and let $x\in X$. Then the set $C_x:=\{\sigma\in X^\omega\mid\sigma(0)=x\}$ is a cone; we assert that its complement cannot be written as a finite union of cones. This is a proof of negation, so suppose to the contrary that $E$ is a finite subset of $X^{<\omega}$ with the property that
\begin{equation*}
C_E:=\bigcup\nolimits_{e\in E}[e]=X^\omega\setminus C_x.
\end{equation*}
Now consider the function $z:X^\omega\to X$ given by $\sigma\mapsto\sigma(0)$. Of course, the image of $z$ is $X$ itself. However, since $X^\omega=C_E\cup C_x$ we have that $X=f(C_E)\cup f(C_x)$. The image of $C_E$ is the set $\{e(0)\mid e\in E\}$, which is finite since $E$ is finite. Hence we have that $X=\{x\}\cup\{e(0)\mid e\in E\}$, contradicting the assumption that $X$ is infinite. Therefore we conclude that the complement of $C_x$ is not a finite union of cones, and consequently that the collection of finite union of cones is not a field.

Back to the case of $2^\omega$; we have to show that $\mathcal{C}$ is exactly the set $\mathcal{C}'$ of finite unions of cylinders. First, note that every cone is a cylinder, so we have that $\mathcal{C}\subseteq\mathcal{C}'$. For the other direction, we will have to write every cylinder as a finite union of cones. Let $e$ be a finite partial function from $\omega$ to $X$ (so that $[e]$ is exactly a general cylinder) and let $L$ be the largest number on which $e$ is defined. We may now consider the set
\begin{equation*}
E:=\{\sigma_L\in 2^L\mid\sigma\in[e]\},
\end{equation*}  
where again $\sigma_L$ is the initial segment of length $L$ of $\sigma$. Then it is immediate that $[e]=\bigcup_{e'\in E}[e']$, which is a finite union of cones. It follows that every finite union of cylinders is a finite union of cones as well, so that we have $\mathcal{C}'\subseteq\mathcal{C}$. This completes the proof that $\mathcal{C}=\mathcal{C}'$.  
\end{ex}

\begin{ex}
We borrow the notation from the solution of exercise 2 above.

For this exercise we have to show that $\{z\}\notin\mathcal{C}$ for the function $z\in 2^\omega$ given by $z(n):=0$. Suppose it were; then we would have a finite subset $E\subseteq 2^{<\omega}$ such that $C_E=\{z\}$. Note that $E$ cannot be empty because in that case we would have $C_E=2^\omega$. Thus, we have some $e\in E$. Now we define $\sigma\in 2^\omega$ by $\sigma(n):=e(n)$ for $n\leq l(e)$ and $\sigma(n):=1$ for larger $n$. Then we have $\sigma_{l(e)}=e$ and hence $\sigma\in C_E$, contradicting with the assumption that $C_E=\{z\}$. Therefore we conclude that it cannot be the case that $\{z\}\in\mathcal{C}$. 
\end{ex}

\begin{ex}
Define
\begin{equation*}
A:=\Big\{\sigma\in 2^\omega\mathbin{\Big|}
      \forall_{(m\in\mathbb{N})}\exists_{(N\in\mathbb{N})}\forall_{(n\geq N)}\,\big|\frac{1}{2}-\frac{1}{n+1}\sum_{i=0}^n(1-\sigma(i))\big|<\frac{1}{m}\Big\}
\end{equation*}
This set represents the proposition that the limiting relative frequency of outcome $0$ is exactly $\frac{1}{2}$. We will first show that this is a measurable set. Note that the quantifiers can be taken out of the set delimiters, i.e.~we have the equality
\begin{equation*}
A=\bigcap_{m\in\mathbb{N}}\bigcup_{N\in\mathbb{N}}\bigcap_{n\geq N} A_{m,N,n},
\end{equation*}
where
\begin{equation*}
A_{m,N,n}:=\Big\{\sigma\in 2^\omega\mathbin{\Big|}\big|\frac{1}{2}-\frac{1}{n+1}\sum_{i=0}^n(1-\sigma(i))\big|<\frac{1}{m}\Big\}
\end{equation*}
The condition in the set $A_{m,N,n}$ is observable because it only involves the outcomes $\sigma(0),\ldots,\sigma(n)$, so these sets are in $\mathcal{C}$. Therefore, we see that $A$ is measurable. 

To see that $A$ is not observable, suppose that $E\subset 2^{<\omega}$ is finite and let $L$ be the maximum length of elements of $E$. We may replace $E$ with $E^\ast\subset 2^L$ as we did in exercise 2. Since we can extend each $e\in 2^L$ with zeroes all the way, we see that no $[e]$ can be contained in $A$. Therefore, $C_{E^\ast}$ cannot be equal to $A$.

(Another strategy which I have tried -- but it took me too long to come up with something -- was to show that $A$ has measure $1$ with the usual measure. This would lead to a solution since the only set in $\mathcal{C}$ with measure $1$ is $2^\omega$ itself.)
\end{ex}

\begin{ex}
Let $\sigma\in 2^\omega$ and write $\sigma_n$ for the initial segment of $\sigma$ of length $n$. Then we have
\begin{equation*}
\bigcap\nolimits_{n\in\omega}[\sigma_n]=\{\sigma\}.
\end{equation*}
In particular, the singleton $\{\sigma\}$ is a measurable set. Note that $[\sigma_{n+1}]\subseteq[\sigma_n]$ for each $n\in\omega$. By the downwards continuity of probability measures, we have that $P(\{\sigma\})=\lim_{n\to\infty}P([\sigma_n])=\lim_{n\to\infty}2^{-n}=0$. 
\end{ex}

\begin{ex}
Let $X$ be a probability space in which singletons are measurable and have measure $0$. Then, for each $x\in X$ we have $P(X\setminus\{x\})=1$. If we assume that arbitrary intersections of sets of measure $1$ are again of measure $1$, then we find that
\begin{equation*}\textstyle
1=P(\bigcap_{x\in X}X\setminus\{x\})=P(\varnothing)\leq P(\{x\})=0,
\end{equation*}
a contradiction.
\end{ex}

\begin{ex}
The empty set is the union of the empty collection of half-open intervals and its complement $\Omega$ is itself a half-open interval. Now let $D$ be a finite set of half-open intervals. From $D$, we
can easily construct a finite set $E$ of pairwise disjoint half-open intervals. 
Write $I_E:=\bigcup_{J\in E} J$ and write $i_E:=\{\inf(J)\in\mathbb{R}\mid J\in E\}$ and $m_E:=\{\max J\in\mathbb{R}\mid J\in E\}$. Then we define 
\begin{align*}
m_E^\ast & :=(i_E\cup\{1\})\setminus (m_E\cup\{0\})\\
i_E^\ast & :=(m_E\cup\{0\})\setminus (i_E\cup\{1\}).
\end{align*} 

Now we assert that we can construct a bijection $f$ from $i_E^\ast$ to $m_E^\ast$. Note that $i_E$ and $m_E$ have the same number of elements because they correspond to beginning and end points of the intervals in $E$. Since $0$ cannot appear as an endpoint and $1$ cannot appear as a beginning point of an interval, it follows that also $i_E\cup\{1\}$ and $m_E\cup\{0\}$ have the same number of elements. Consequently, $i_E^\ast$ and $m_E^\ast$ have the same number of elements. Now the elements of $i_E^\ast$ and $m_E^\ast$ can be ordered (the natural way) and we let the bijection $f$ be the unique order preserving bijection.

This allows us to consider the finite collection
\begin{equation*}
F=\{(x,f(x)]\mid x\in i_E^\ast\}
\end{equation*}
of half-open intervals. We have made sure that each of these intervals is disjoint from any of the intervals in $E$ (note that the half-open intervals in $E$ do not intersect each other). By including $0$ and $1$ at the appropriate moments, we have made sure that $\bigcup F$ is indeed the complement of $\bigcup E$.

If we would have taken open intervals, the class $\mathcal{H}$ would not be closed under complement because comlements of open sets are closed and the interval is connected.
\end{ex}

\begin{ex}
It depends on what you want to measure. If you're in the plane and want to point to the square, the answer will be different than if you're in space and want to lay another square over the hidden square.
\end{ex}

\setcounter{ex}{0}

\begin{ex}
The probability of looking at the `heads' side of a coin is $\frac{1}{2}$. The probability of looking at the `heads' side of the coin which has `tails' on the other side is $\frac{1}{6}$. Thus, the conditional probability that tais is on the other side in the event that  we flipped heads on a random coin is $\frac{1}{3}$.
\end{ex}

\begin{ex}
If $H$ entails $E$, then $H\subseteq E$. So $P(H|E)=P(H\cap E)/P(E)=P(H)/P(E)>P(H)$. The latter step is justified by $P(E)<1$. If $H$ entails $\neg E$, then $H\subseteq E=\varnothing$. Therefore, we have $P(H|E)=P(H\cap E)/P(E)=P(\varnothing)/P(E)=0/P(E)=0$.
\end{ex}

\begin{ex}
The error is that the choice of the prisoners to be released has been made independent of him knowing part of the outcome, while in his reasoning he worries about a dependency. The formula for conditional probability has to be used instead to find the correct answer.
\end{ex}

\begin{ex}
There exists a cow which is not white, so the probability that all cows are white \emph{at present} is true is $0$ and is independent of whatever crows are observed.
\end{ex}

\begin{ex}
If $X$ is a set with $n$ elements in which each $x\in X$ has probability $1/n$,
then the natural way to define a probability measure on $X^m$ is by assigning
to each outcome a probability of $1/n^m$. Now we can calculate 
$P(\sigma_i=x|\sigma_j=y)$, provided that $i\neq j$ by the usual formula for 
conditional probability:
\begin{equation*}
P(\sigma_i=x|\sigma_j=y)=\frac{n^{m-2}/n^m}{n^{m-1}/n^m}=1/n
\end{equation*}
Thus, we see that the outcome at $i$ is independent of the outcome at $j$ for
any two different $i$ and $j$. In particular, this is so for $X=2$, $m=3$ and $j=3$.

There are four possible frequencies. The probability that the first two outcomes
are $0$ is $\frac{1}{4}+\frac{1}{3}\cdot\frac{1}{4}=\frac{1}{3}$. The probability
that all outcomes are $0$ is $\frac{1}{4}$, so the conditional probability that
the third outcome will be $0$ in the event that the first two were already $0$
is $\frac{3}{4}$.

This does not justify induction at all, or we would be able to prove Riemann's
hypothesis simply by observing that the first many non-trivial zeroes of the
$\zeta$-function have indeed real part $\frac{1}{2}$. 
\end{ex}

\begin{ex}
The probability that components in series compose an operating electrical system is the product of the probabilities that each of the components in that series operates. The probability that components in parallel compose an operating electrical system is $1$ minus the product of the probabilites that each component does not operate. The class of all systems is inductively generated by composing in parallel and in series, so this suggests that we can break this problem down to smaller systems in order to make the calculation.

The probability in which machine $1$ operates is $p$. The probability in which machine $3$ operates is $1-(1-p)^2$. The probability in which machine $2$ operates is $1-(1-p)\cdot(1-(1-(1-p)^3)\cdot p)$. The probability that the electrical system operates is therefore
\begin{equation*}
p\cdot(1-(1-p)\cdot(1-(1-(1-p)^3)\cdot p)\cdot(1-(1-p)^2).
\end{equation*}
\end{ex}

\end{document}
